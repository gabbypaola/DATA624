---
title: "DATA 624 | Predictive Analytics"
author: "Gabriella Martinez"
date: "`r Sys.Date()`"
output:
      html_document:
        toc: yes
        toc_float: yes
        theme: yeti
        highlight: kate
        font-family: "Arial"
        code_folding: hide
---

# Project 1
## Instructions

This project consists of 3 parts - two required and one bonus and is worth 15% of your grade. The project is due at 11:59 PM on Sunday Apr 3. I will accept late submissions with a penalty until the meetup after that when we review some projects.

## Packages
```{r message=FALSE, warning=FALSE, class.source = 'fold-show'}
library(readxl)
library(fpp3)
library(forecast)
library(ggplot2)
library(imputeTS)
library(readr)
```

## Part A – ATM Forecast
 
In part A, I want you to forecast how much cash is taken out of 4 different ATM machines for May 2010. The data is given in a single file.  The variable ‘Cash’ is provided in hundreds of dollars, other than that it is straight forward. I am being somewhat ambiguous on purpose to make this have a little more business feeling.  

- Explain and demonstrate your process, techniques used and not used, and your actual forecast.   
- I am giving you data via an excel file, please provide your written report on your findings, visuals, discussion and your R code via an RPubs link along with the actual.rmd file  
- Also please submit the forecast which you will put in an Excel readable file.
 

### Load Data
```{r message=FALSE, warning=FALSE}
#read in data and specify column types
atm_raw <- read_xlsx("ATM624Data.xlsx", col_types = c("date", "text", "numeric")) 

#, col_types = c("date", "text", "numeric")

str(atm_raw)
```

Based on the above, clean up is required before converting the tibble to a tsibble object for forecasting. Additionally, ATM4 shows values in terms of decimals even after accounting for the hundreds which suggests odd amounts including dollars and cents not typically dispensed by ATMs.

```{r}
#filter raw data for ATM4
atm_raw %>% 
  filter(ATM == "ATM4")%>% 
  head(10) 
```

### Exploratory Data Analysis

```{r message=FALSE, warning=FALSE}
#distro plot
atm_raw %>% 
  filter(DATE < "2010-05-01", !is.na(ATM)) %>% 
  ggplot(aes(x = Cash)) +
    geom_histogram(bins = 30) +
    facet_wrap(~ ATM, ncol = 2, scales = "free") +
    labs(title = "Daily ATM Withdrawals") +
    scale_y_continuous("Withdrawals (hundreds)")


#ts plot
atm_raw %>% 
  filter(DATE < "2010-05-01", !is.na(ATM)) %>% 
  ggplot(aes(x = DATE, y = Cash, col = ATM)) +
    geom_line(show.legend = FALSE) +
    facet_wrap(~ ATM, ncol = 1, scales = "free_y") +
    labs(title = "Daily ATM Withdrawals", x = "Date") +
    scale_y_continuous("Withdrawals (hundreds)")
```

```{r}
atm <- atm_raw %>% 
  #lowercase column names
  rename(date=DATE, atm=ATM, cash=Cash) %>%
  #convert date column from POSIXct to date type , cash = cash*100
  mutate(date = as.Date(date)) %>% 
  #convert from long to wide by expanding the atm column to get individual column for each atm
  pivot_wider(names_from=atm, values_from = cash) %>% 
  #remove the NA column
  select(-"NA") %>% 
  #filter out what we will be forecasting
  filter(date < "2010-05-01") %>% 
  #convert to tsibble
  as_tsibble(index=date) 

head(atm)
```

#### NA value detection

Next we determine how many NAs we are dealing with and identify which ATMs have the NAs and on which dates. Using the below, both ATMs 1 and 2 seem to have all the NA values and all happen around the month of June. 

```{r}
#how man NA values
apply(is.na(atm),2,sum)

#what dates are NAs present
atm[!complete.cases(atm), ]

#unique(atm$ATM3)

#sum(is.na(atm_raw$ATM))
```

#### Summary statistics
```{r}
#summary stats
summary(atm)
```

- ATM1: Contains 3 NA values. The series shows a seasonal pattern with no trend where the time series is affected by the day of the week. The distribution of the cash values is bimodal and are left skewed.

- ATM2: Contains 2 NA values. Similar to ATM1, the series shows a seasonal pattern with no trend where the time series is affected by the day of the week. The distribution of the cash values are left skewed.

- ATM3: There are only three dates with amounts which are 04/28/2010, 04/29/2010, and 04/30/2010, all other dates have cash values of 0. It is not worth conducting a forecast for ATM3 given the lack of daily values. A potential reason for lack of data might be that the ATM was recently placed and therefore has not accumulated transactions for the time period.

- ATM4: ATM4 shows seasonality with no trend and an extreme outlier that skews the time series plot, by a value of 10919.762. The distribution of the cash values are right skewed.
At first glance of the values of ATM4 are quite odd although the values are expressed in hundreds. ATMs typically dispense money in denominations of 50s and 20s, particularly those serviced by banks and bank owned ATMs serviced by amoured car cash management services. Additionally, for small ATMs serviced by store fronts similar to those found convenience stores, those output cash in additional denominations ranging from 50s, 20s, 10s and in some cases 5s. For the purpose of this project, the data will be rounded to shave off the additional values suggesting odd dollar and coin amounts. Lastly, the values in general are very large for one ATM. For example, the first observation o 05/01/2009, shows a value of 777, which after expressing in hundreds is 777,000. That is over three quarters of a million of dollars which is an exorbitant amount of one ATM to hold. This value may be reflective of multiple ATMs in one vestibule, and not one singular ATM.

```{r}
atm$ATM4 <- round(atm$ATM4, 0)

head(atm)
```



### Missing Values and Outliers

Using the `na.interp()` function from the forecast library, the missing values for ATM1 and ATM2 are generated through interpolation. "For seasonal series, a robust STL decomposition is first computed. Then a linear interpolation is applied to the seasonally adjusted data, and the seasonal component is added back." ^[https://pkg.robjhyndman.com/forecast/reference/na.interp.html] The `na.interp()` was chosen over a mean method because the distributions for both ATM1 and ATM2 were skewed and the values are seasonal based on day of the week, so using a mean might not provide as accurate output as an interpolation method.

```{r, class.source = 'fold-show'}
#generate estimates for NAs and replace with new values

#ATM1
atm$ATM1 <- na.interp(atm$ATM1)

#ATM2 
atm$ATM2 <-  na.interp(atm$ATM2)

```

Next, using the `tsoutliers()`^[https://pkg.robjhyndman.com/forecast/reference/tsoutliers.html] function from the forecast library, we identify and replace the outlier skewing the data for ATM4. 
```{r, class.source = 'fold-show'}
#ATM4

#identify the outlier index and generate a replacement 
#tsoutliers(atm$ATM4)

#do the replacement
atm$ATM4[285] <- tsoutliers(atm$ATM4)$replacements
```

Given the slight skews detected for ATMs 1 and 2, we will also use `tsoutliers()` to detect any existing outliers and replace them accordingly. Neither ATM require outlier replacement. 

```{r message=FALSE, warning=FALSE}
tsoutliers(atm$ATM1)

tsoutliers(atm$ATM2)
```

Using the below we see there are no NA values remaining in ATMs 1 and 2, and the outlier for ATM4 has been taken care of. While the outlier for ATM4 has been resolved, there is still a high right skew on the data which is an indicator that a transformation may be required prior to modeling and forecasting.
```{r message=FALSE, warning=FALSE}
#verify how many NA values
apply(is.na(atm),2,sum)

summary(atm)
```

### ATM 1 Forecast

Next, in order to uncover additional patterns noted in the series, an STL "Seasonal and Trend decomposition using Loess" decomposition is performed given the seasonality of the series. The parameter `trend(window = 7)` is set to 7 for the daily data, and the `season(window='periodic')` parameter force the seasonal component to be periodic across days of the week. Both trend and seasonal windows should be odd numbers; trend window is the number of consecutive observations to be used when estimating the trend-cycle; season window is the number of consecutive years to be used in estimating each value in the seasonal component.

```{r message=FALSE, warning=FALSE}
ATM_1 <- atm %>% 
  select(date, ATM1)

ATM_1 %>%
  model(
    STL(ATM1 ~ trend(window = 7) +
                   season(window = "periodic"),
    robust = TRUE)) %>%
  components() %>%
  autoplot()
```

The ACF plot below suggest lags at 2, 5, and 7 with 7 being the most significant lag. The ACF seems to be decreasing relatively slowly which could be an indicator that the data is non-stationary and could potentially benefit from differencing. Although the ACF is decreasing slowly, the $r_1$ value is not very large suggesting that the series might not be stationary. Additional checks are required.

```{r message=FALSE, warning=FALSE}
ATM_1 %>% 
  ACF(ATM1, lag_max = 28) %>% 
  autoplot()
  #gg_tsdisplay(plot_type='partial', lag_max = 28)
```

Following from above and in preparation for an ARIMA model, `ndiffs()` is used to determine if any differencing is required. Based on the output, there is no differencing required.

```{r message=FALSE, warning=FALSE}
ndiffs(ATM_1$ATM1)
```

A KPSS test using `features(df$column, unitroot_kpss)` was considered to further evaluate if the series is stationary or not however a "major disadvantage for the KPSS test is that it has a high rate of Type I errors (it tends to reject the null hypothesis too often);" as such it has been omitted. ^[https://www.statisticshowto.com/kpss-test/#:~:text=What%20is%20the%20KPSS%20Test,variance%20%E2%80%94%20are%20constant%20over%20time.]

#### Model

Given the seasonal nature of the data, a reasonable approach is to include a seasonal naive method where last period’s sales are used for the next period’s forecast without predictions or adjusting the factors. Alternative models for this series are ETS and ARIMA models. An additional model that has been included is the `Auto ARIMA`, which is an automatically selected model by R with optimal values.

```{r fig.height=10, fig.width=15, message=FALSE, warning=FALSE}
#filter train data filter April data out to include it for eval later
train <- ATM_1 %>%
  filter(date < "2010-04-01")

#run seasonal related models
fit <- train %>%
  model(
    SNAIVE = SNAIVE(ATM1),
    ETS = ETS(ATM1),
    ARIMA = ARIMA(ATM1),
    `Auto ARIMA` = ARIMA(ATM1, stepwise = FALSE, approx = FALSE)
  )

#forecast_ATM1 April 2010
forecast_ATM1 <- fit %>%
  forecast(h = 30)

#plot
forecast_ATM1 %>%
  autoplot(ATM_1, level = NULL)+
  facet_wrap( ~ .model, scales = "free_y") +
  guides(colour = guide_legend(title = "Forecast"))+
  labs(title= "ATM1 Forecasts",
       subtitle = "April 2010") +
  xlab("Date") +
  ylab("Cash values in Hundreds") 
```

#### Evaluation

Using the `accuracy()` function, we determine which model performed best based on their metrics. In review of the metrics, the ETS model has higher accuracy on the cross-validated performance measures for ATM1.

```{r message=FALSE, warning=FALSE}
#find RMSE and other metrics
accuracy(forecast_ATM1, ATM_1) %>%
  select(.model, RMSE:MAPE)
```

#### Generate Forecast

Next, the forecast for ATM1 is produced after obtaining the optimal model of the four generated above.

```{r message=FALSE, warning=FALSE}
#reproduce the mode using the original dataset 
AMT1_ETS_fit <- ATM_1 %>% 
  model(
    ETS = ETS(ATM1))

#generate the values
ATM1_EST_forecast <- AMT1_ETS_fit %>% 
  forecast(h=30)

#plot
ATM1_EST_forecast %>% 
  autoplot(ATM_1) +
  labs(title = "ATM1 - ETS Forecast",
       subtitle = "May 2010",
       y = "Cash Values in Hundreds")
```

```{r}
#convert to df and select only the columns needed
ATM1_forecast_values <- as.data.frame(ATM1_EST_forecast) %>% 
  select(date, .mean)

#fix column names
colnames(ATM1_forecast_values) <- c('Date', 'Cash')

#back transform values
ATM1_forecast_values <- ATM1_forecast_values %>% 
    mutate(Cash= round(Cash))

ATM1_forecast_values

#save to csv
write_excel_csv(ATM1_forecast_values, "ATM1_forecast.csv")
```


### ATM 2 Forecast

Similar to the process for ATM1, an STL "Seasonal and Trend decomposition using Loess" decomposition is performed to uncover any additional patterns in each component. 

```{r message=FALSE, warning=FALSE}
ATM_2 <- atm %>% 
  select(date, ATM2)

ATM_2 %>%
  model(
    STL(ATM2 ~ trend(window = 7) +
                   season(window = "periodic"),
    robust = TRUE)) %>%
  components() %>%
  autoplot()
```

The ACF plot below suggest lags at 2, 5, and 7 with 7 being the most significant lag. The ACF seems to be decreasing relatively slowly which could be an indicator that the data is non-stationary and could potentially benefit from differencing. Although the ACF is decreasing slowly, the $r_1$ value is not very large suggesting that the series might not be stationary. Additional checks are required.

```{r message=FALSE, warning=FALSE}
ATM_2 %>% 
  ACF(ATM2, lag_max = 28) %>% 
  autoplot()
```

Following from above and in preparation for an ARIMA model, `ndiffs()` is used to determine if any differencing is required. Based on the output, differencing required.

```{r}
ndiffs(ATM_2$ATM2)
```


#### Model

In the case of ATM2, 2 separate fits are generated. One fit with two models, SNAIVE and ETS, is generated using the non-differenced data. The other fit with three models, ETS, ARIMA, and Auto Arima, is generated using the differenced data.
 
```{r fig.height=8, fig.width=15, message=FALSE, warning=FALSE}
#need to add differenced data
ATM_2 <- ATM_2 %>% 
  mutate(diff_ATM2= difference(ATM2))

#filter train data filter April data out to include it for eval later
train <- ATM_2 %>%
  filter(date <= "2010-04-01")

#run seasonal related models without the differenced data
fit_1 <- train %>%
  model(
    SNAIVE = SNAIVE(ATM2),
    ETS = ETS(ATM2),
  )

#run models with differenced data
fit_2 <- train %>%
  slice(2:336) %>% 
  model(
    ETS_diff = ETS(diff_ATM2),
    ARIMA = ARIMA(diff_ATM2),
   `Auto ARIMA` = ARIMA(diff_ATM2, stepwise = FALSE, approx = FALSE)
  )

#forecast_ATM2 April
forecast_ATM2_1 <- fit_1 %>%
  forecast(h = 30)

#forecast_ATM2 April
forecast_ATM2_2 <- fit_2 %>%
  forecast(h = 30)

#plot
forecast_ATM2_1 %>%
  autoplot(ATM_2, level = NULL)+
  facet_wrap( ~ .model, scales = "free_y") +
  guides(colour = guide_legend(title = "Forecast"))+
  labs(title= "ATM2 Forecasts",
       subtitle = "April 2010") +
  xlab("Date") +
  ylab("Cash values in Hundreds") 

#plot 2
forecast_ATM2_2 %>%
  autoplot(ATM_2, level = NULL)+
  facet_wrap( ~ .model, scales = "free_y") +
  guides(colour = guide_legend(title = "Forecast"))+
  labs(title= "ATM2 Forecasts",
       subtitle = "April 2010") +
  xlab("Date") +
  ylab("Cash")
```


#### Evaluate

Next the `accuracy()` function is run on both forecast objects to obtain the best fit based on the metrics. Of the five models run, the ETS model without the differenced data performs the best.

```{r message=FALSE, warning=FALSE}
#find RMSE and other metrics
accuracy(forecast_ATM2_1, ATM_2) %>%
  select(.model, RMSE:MAE)

accuracy(forecast_ATM2_2, ATM_2) %>%
  select(.model, RMSE:MAE)

```

#### Generate Forecast

Below the forcast is generated and plotted.

```{r message=FALSE, warning=FALSE}
#reproduce the mode using the original dataset 
AMT2_ETS_fit <- ATM_2 %>% 
  model(
    ETS = ETS(ATM2))

#generate the values
ATM2_EST_forecast <- AMT2_ETS_fit %>% 
  forecast(h=30)

#plot
ATM2_EST_forecast %>% 
  autoplot(ATM_2) +
  labs(title = "ATM2 - ETS Forecast",
       subtitle = "May 2010",
       y = "Cash Values in Hundreds")
```

```{r}
#convert to df and select only the columns needed
ATM2_forecast_values <- as.data.frame(ATM2_EST_forecast) %>% 
  select(date, .mean)

#fix column names
colnames(ATM2_forecast_values) <- c('Date', 'Cash')

#round values for integer output
ATM2_forecast_values <- ATM2_forecast_values %>% 
    mutate(Cash= round(Cash))

ATM2_forecast_values

#save to csv
write_excel_csv(ATM2_forecast_values, "ATM2_forecast.csv")
```


### ATM 4 Forecast

As seen below in the plot, ATM4 has the most variance of all ATMs modeled thus far; as such a mathematical transformation will be required.

```{r message=FALSE, warning=FALSE}
ATM_4 <- atm %>% 
  select(date, ATM4)

ATM_4 %>%
  model(
    STL(ATM4 ~ trend(window = 7) +
                   season(window = "periodic"),
    robust = TRUE)) %>%
  components() %>%
  autoplot()
```

Next, the lambda value is generated along with adding the transformed values for modeling, evaluation, and forecasting. Lambda turns out to be 0.4 which is approximately a square root transform. In this case, a square root transform is prefered for simplicity of the back transform in the final forecast values as opposed to an actual Box-Cox transform.

```{r message=FALSE, warning=FALSE}
#get lambda
lambda <- ATM_4 %>%
  features(ATM4, features = guerrero) %>%
  pull(lambda_guerrero)

#add new value- sqrt transform
ATM_4 <- ATM_4 %>% 
    mutate(ATM4_sqrt= sqrt(ATM4))

#plot
ATM_4 %>% 
  autoplot(ATM4_sqrt) +
  labs(y = "Cash Values in Hundreds",
       title ="Square root Transformed ATM4 Cash Values")
```

The summary of the Box-Cox transformed data suggests the skew identified above has decreased significantly in comparison to the earlier summary statistics run on the untransformed data. The transformed data is almost normal, with a slight left skew given the mean is left of the median by a relatively small amount.

```{r message=FALSE, warning=FALSE}
summary(ATM_4$ATM4_sqrt)
```


The ACF plot below suggest lags only at 7. The ACF seems to be decreasing relatively slowly, but after close inspection it looks like the ACF decreases from lags 7 to 21, and then slightly shifts back up at 28. Given the shift, an additional check will be performed to see if the series requires differencing.

```{r message=FALSE, warning=FALSE}
ATM_4 %>% 
  ACF(ATM4_sqrt, lag_max = 28) %>% 
  autoplot()
```

Following from above and in preparation for an ARIMA model, `ndiffs()` is used to determine if any differencing is required. Based on the output, there is no differencing required.

```{r message=FALSE, warning=FALSE}
ndiffs(ATM_4$ATM4)

ndiffs(ATM_4$ATM4_sqrt)
```

#### Model

Four different models are run for ATM4 SNAIVE, ETS, ARIMA, and an Auto ARIMA.

```{r fig.height=8, fig.width=15, message=FALSE, warning=FALSE}
#filter train data filter April data out to include it for eval later
train <- ATM_4 %>%
  filter(date < "2010-04-01")

#run seasonal related models
fit <- train %>%
  model(
    SNAIVE = SNAIVE(ATM4_sqrt),
    ETS = ETS(ATM4_sqrt),
    ARIMA = ARIMA(ATM4_sqrt),
    `Auto ARIMA` = ARIMA(ATM4_sqrt, stepwise = FALSE, approx = FALSE)
  )

#forecast_ATM1 April 2010
forecast_ATM4 <- fit %>%
  forecast(h = 30)

#plot
forecast_ATM4 %>%
  autoplot(ATM_4, level = NULL)+
  facet_wrap( ~ .model, scales = "free_y") +
  guides(colour = guide_legend(title = "Forecast"))+
  labs(title= "ATM4 Forecasts",
       subtitle = "April 2010") +
  xlab("Date") +
  ylab("Cash values in Hundreds") 
```


#### Evaluation

The `accuracy()` function is used to determine which model performed best based on their metrics. In review of the metrics, the Auto ARIMA model has higher accuracy on the cross-validated performance measures with exception to the SNAIVE which has a better performing MAE.

```{r message=FALSE, warning=FALSE}
#find RMSE and other metrics
accuracy(forecast_ATM4, ATM_4) %>%
  select(.model, RMSE:MAE)
```



#### Generate Forecast

Finally the forecast for ATM4 is produced using the optimal model method uncovered in the previous step. In the case of ATM4, the best model is ETS. The final values are expressed using the square root transformed values and will be back transformed.

```{r message=FALSE, warning=FALSE}
#reproduce the mode using the original dataset 
AMT4_SN_fit <- ATM_4 %>% 
  model(ETS = ETS(ATM4_sqrt))

#generate the values
ATM4_SN_forecast <- AMT4_SN_fit %>% 
  forecast(h=30)

#plot
ATM4_SN_forecast %>% 
  autoplot(ATM_4) +
  labs(title = "ATM4 - ETS Forecast",
       subtitle = "May 2010",
       y = "Cash Values in Hundreds")
```

Lastly, the values of the forecast are saved into .CSV format for submission after being back transformed by squaring and rounding the values.

```{r}
#convert to df and select only the columns needed
ATM4_forecast_values <- as.data.frame(ATM4_SN_forecast) %>% 
  select(date, .mean)

#fix column names
colnames(ATM4_forecast_values) <- c('Date', 'Cash')

#back transform values
ATM4_forecast_values <- ATM4_forecast_values %>% 
    mutate(Cash= round(Cash**2))

ATM4_forecast_values

#save to csv
write_excel_csv(ATM4_forecast_values, "ATM4_forecast.csv")
```

## Part B – Forecasting Power
 
Part B consists of a simple dataset of residential power usage for January 1998 until December 2013.  Your assignment is to model these data and a monthly forecast for 2014.  The data is given in a single file.  The variable ‘KWH’ is power consumption in Kilowatt hours, the rest is straight forward. Add this to your existing files above. 

### Load Data

```{r message=FALSE, warning=FALSE}
power_raw <- read_xlsx("ResidentialCustomerForecastLoad-624.xlsx")

str(power_raw)
```

After inspecting the structure of our `power_raw`, it looks like there are three columns, one of which is irrelevant (`CaseSequence`) to our forecast of power utilization. `CaseSequence` will be removed in the cleaning process. The `YYYY-MM` column will need to be converted to a date type in order to be used as the index for the required tsibble object. 

Using `summary`, there is also one NA in the `KWH` column, and a potential outlier in the minimum value. Additionally, the distribution shows a right skew since the mean is considerably larger than the median.
 
```{r}
summary(power_raw)
```

```{r}
#what month is the NA at
power_raw[!complete.cases(power_raw), ]

#what month is the min at
power_raw %>% 
  filter(KWH == 770523)
```

Next we clean the data to adjust the variable type of `YYYY-MM` to date type and call it `Month` and create a `tsibble` object from the `tibble` Additonally, the `KWH` variable is transformed to be expressed in values of thousands to make it easier for analysis and interpretation.

```{r message=FALSE, warning=FALSE}
#change variable type 
power_tsibble <- power_raw %>% 
  mutate(Month = yearmonth(`YYYY-MMM`), KWH = KWH/1000) %>%
  select(-CaseSequence, -'YYYY-MMM') %>% 
  tsibble(index= Month)

head(power_tsibble)
```


### Exploratory Data Analysis

The distribution plot confirms the initial finding a right skew using the mean and median. Additionally, the outlier detected above can also be seen on the far left of the plot. The outlier will need to be imputed and replaced. 

Next, the timeseries plot shows the NA value initially detected with the `summary functon` in September of 2008. The outlier can also clearly be seen in the plot in July of 2010. As mentioned previously both will require replacement in order to conduct a model and forecast. 

```{r message=FALSE, warning=FALSE}
#distro plot
ggplot(power_tsibble, aes(x=KWH))+
  geom_histogram(bins=40)+
  labs(title = "Distribution of Monthly Residential Power Usage",
       subtitle="January 1998 - December 2013") +
  ylab(label="")

#ts plot
power_tsibble %>%
  autoplot(KWH) +
  labs(title = "Monthly Residential Power Usage",
       subtitle="January 1998 - December 2013")
```

### Missing Values and Outliers

First a copy of the table is created, then the NA value is replaced using `na.interp`. Next, `tsoutliers` is used on the outlier identified above to detect and replace it. For some reason, the `tsoutliers` function didn't detect the outlier seen in the distribution to the far left. Instead of using the function to replace the outlier, it will be manually replaced with the median in order not to further skew the distribution by changing it using the mean.

```{r class.source = 'fold-show'}
#create a copy of the tsibble
power <- power_tsibble

#replace NA
power$KWH <- na.interp(power$KWH)

#find and detect outlier - didn't work
tsoutliers(power$KWH)

#manually change the outlier
power$KWH <- replace(power$KWH, power$KWH == min(power$KWH),
                          median(power$KWH))
```

Below is the distribution after resolving the NA value and the outlier, it still has a right skew. The time series plot of the variable shows strong seasonality with an upward trend picking up around 2008-2009. Given the skew of the data, a Box-Cox transformation might be useful.

```{r message=FALSE, warning=FALSE}
#distro histo plot
ggplot(power, aes(x=KWH))+
  geom_histogram()+
  labs(title = "Distribution of Monthly Residential Power Usage",
       subtitle="January 1998 - December 2013") +
  ylab(label="")

#summary
summary(power$KWH)

#ts plot
power %>%
  autoplot(KWH) +
  labs(title = "Monthly Residential Power Usage",
       subtitle="January 1998 - December 2013")+
  ylab(label= "KWH in Thousands")
```

### Transformation

Below is the `summary()` for the Box-Cox transformed `power` data set which now has a normal distribution noted by the mean and median being equal; time series plot doesn't look extremely different from the original time series plot besides the scale now being smaller and slightly less variance. 

```{r message=FALSE, warning=FALSE}
#get lambda
lambda <- power %>%
  features(KWH, features = guerrero) %>%
  pull(lambda_guerrero)

#add new value
power <- power %>% 
    mutate(KWH_bc = box_cox(KWH, lambda))

#stats
summary(power$KWH_bc)

#distro histo plot
ggplot(power, aes(x=KWH_bc))+
  geom_histogram()+
  labs(title = "Distribution of Monthly Residential Power Usage",
       subtitle="January 1998 - December 2013") +
  ylab(label="")

#plot
power %>% 
  autoplot(KWH_bc) +
  labs(y = "KWH in Thousands",
       title = latex2exp::TeX(paste0(
         "Transformed KWH with $\\lambda$ = ",
         round(lambda,2))))
```

### Forecast

To uncover additional patterns noted in the series, an STL "Seasonal and Trend decomposition using Loess" decomposition is performed given the seasonality of the series. Below we see the yearly seasonality and a trend appearing towards the end of the series.

```{r}
power %>%
  model(
    STL(KWH_bc ~ trend(window = 13) +
                   season(window = "periodic"),
    robust = TRUE)) %>%
  components() %>%
  autoplot()
```

The ACF plot below tells us:

- $r_{12}$ is the highest of all the lags and is due to the seasonal pattern for the data. The highest lags are at every 12th r value.
- $r_{3}$ is the first highest negative lag and has a peak every 6 months onward (9,15,21,27,33). The troughs tend to be six months behind peaks.
- All of the correlations plotted are significantly different from zero as they all fall outside of the dashed blue lines.
- The 12 month lags appear to be decreasing relatively slowly which could be an indicator that the data is non-stationary and could potentially benefit from differencing. Additional check using `ndiffs()` will follow.

```{r}
power %>% 
  ACF(KWH_bc, lag_max = 36) %>% 
  autoplot()
```

One differencing will be required. A new tsibble is created which contains only the `KWH` and Box-Cox transformed `KWH_bc` variable. The first observation is removed using `slice()` because it contains NA values for `KWH` and `KWH_bc` resulting from the differencing performed. The `ndiffs()` function is run again to confirm the new tsibble created does not need another round of differencing.

```{r}
ndiffs(power$KWH_bc)

#need to add differenced data
diff_power <- power %>% 
  mutate(diff_KWH= difference(KWH), diff_KWH_bc = difference(KWH_bc)) %>% 
  #remove unecessary columns
  select(-KWH, -KWH_bc) %>% 
  #first observation is NA due to differencing so remove
  slice(-1)

ndiffs(diff_power$diff_KWH_bc)
```

### Model

Next the `diff_power` is split to create a training data set, `diff_train`. The power data is highly seasonal and resembles the high seasonality noted in the previous problem. As such, a similar approach will be taken in terms of running the same models, however use of a Holt's-Winters' additive ETS model has been included. 

```{r fig.height=10, fig.width=15, message=FALSE, warning=FALSE}
#Differenced data for arima

#split
diff_train <- diff_power %>% 
  filter(year(Month) < 2013)

#models
diff_fit <- diff_train %>% 
    model(
    ARIMA = ARIMA(diff_KWH),
    `Auto ARIMA` = ARIMA(diff_KWH, stepwise = FALSE, approx = FALSE)
  )

#forecast of 2013
diff_fc <- diff_fit %>% 
  forecast(h = "1 year")

#plot
diff_fc %>%
  autoplot(diff_power, level = NULL)+
  facet_wrap( ~ .model, scales = "free_y") +
  guides(colour = guide_legend(title = "Forecast"))+
  labs(title= "KWH Forecasts",
       subtitle = "January 2013 - December 2013")+
  xlab("Month") +
  ylab("KWH in Thousands") 
```

```{r fig.height=10, fig.width=15, message=FALSE, warning=FALSE}
#split
train <- power %>% 
  filter(year(Month) < 2013)

#models
fit <- train %>% 
    model(
    ETS = ETS(KWH),
    `Additive ETS` = ETS(KWH ~ error("A") + trend("A") + season("A")),
    SNAIVE = SNAIVE(KWH)
  )

#forecast of 2013
fc <- fit %>% 
  forecast(h = "1 year")

#plot
fc %>%
  autoplot(power, level = NULL)+
  facet_wrap( ~ .model, scales = "free_y") +
  guides(colour = guide_legend(title = "Forecast"))+
  labs(title= "KWH Forecasts",
       subtitle = "January 2013 - December 2013")+
  xlab("Month") +
  ylab("KWH in Thousands") 
```


### Evaluate

The `accuracy()` function is used to determine which model performed best based on their metrics. In review of the metrics, the Additive ETS model out performed all other models with lower RMSE with exception to the SNAIVE that has a the lowest MAE.

```{r}
#find ARIMA RMSE, MAE
accuracy(diff_fc, diff_power) %>%
  select(.model, RMSE:MAE)

#find other RMSE, MAE
accuracy(fc, power) %>%
  select(.model, RMSE:MAE)
```


### Forecast

Finally the forecast values and plot are generated using the Additive ETS Forecast.

```{r message=FALSE, warning=FALSE}
#reproduce the mode using the original dataset 
power_ETS_fit <- power %>% 
  model(`Additive ETS` = ETS(KWH ~ error("A") + trend("A") + season("A")))

#generate the values
power_EST_forecast <- power_ETS_fit %>% 
  forecast(h=12)

#plot
power_EST_forecast %>% 
  autoplot(power) +
  labs(title = "Monthly Residential Power Usage",
       subtitle = "Additive ETS Forecast 2014",
       y = "KWH in Thousands")
```

Lastly, the values of the forecast are saved into .CSV format for submission.

```{r}
#convert to df and select only the columns needed
power_forecast_values <- as.data.frame(power_EST_forecast) %>% 
  select(Month, .mean)

#fix column names
colnames(power_forecast_values) <- c('Month', 'KWH Forecast')

power_forecast_values

#save to csv
write_excel_csv(power_forecast_values, "power_forecast.csv")
```


## References


<!------- Below is for removing excessive space in Rmarkdown | HTML formatting -------->

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>