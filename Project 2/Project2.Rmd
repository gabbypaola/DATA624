---
title: "DATA 624 | Predictive Analytics"
subtitle: "Project 2"
author: "Gehad Gad, Karim Hammoud, Gabriella Martinez"
date: "`r Sys.Date()`"
output:
      html_document:
        toc: yes
        toc_float: yes
        theme: yeti
        highlight: kate
        font-family: "Arial"
        code_folding: hide
---


# Instructions

This is role playing.  I am your new boss.  I am in charge of production at ABC Beverage and you are a team of data scientists reporting to me.  My leadership has told me that new regulations are requiring us to understand our manufacturing process, the predictive factors and be able to report to them our predictive model of pH.  

Please use the historical data set I am providing. Build and report the factors in BOTH a technical and non-technical report.  I like to use Word and Excel.  Please provide your non-technical report in a business friendly readable document and your predictions in an Excel readable format. The technical report should show clearly the models you tested and how you selected your final approach.  

Please submit both Rpubs links and .rmd files or other readable formats for technical and non-technical reports.  Also submit the excel file showing the prediction of your models for pH.
```{r message=FALSE, warning=FALSE}
library(ggplot2)
library(readr)
library(DataExplorer)
library(summarytools)
library(Amelia)
library(VIM)
library(dplyr)
library(forecast)
library(tidyr)
library(mice)
library(corrplot)
library(MASS)
library(earth)
library(RANN)
library(caret)
library(car)
library(forcats)
library(RColorBrewer)
library(randomForest)
library(gbm)
library(Cubist)
library(kableExtra)
library(e1071)
```

# Exploratory Data Analysis

## Load & Review Train Data
```{r message=FALSE, warning=FALSE}
#load data
train <- read.csv("TrainingData.csv")
#review
glimpse(train)
```

From the above it is noted that the `train` data set:

- 2571 observations with 33 columns (32 predictor variables)
- `Brand.Code` is character type and seems it is an unordered categorical variable which needs to be updated as such
- 4 predictor variables are integer type: `Hyd.Pressure4`, `Filler.Speed`, `Carb.Flow`, `Bowl.Setpoint`, remaining are float
- There are predictors with varying ranges for example: `Mnf. Flow` -100.20 to 229.40, `Carb.Flow` 26 to 5104, and `PSC` 0.002 to 0.270
- NAs detected in the first few observations

## Missing Values
```{r message=FALSE, warning=FALSE, results=F}
#NA counts by column
#sapply(train, function(x) sum(is.na(x)))
VIM::aggr(train, numbers=T, sortVars=T, bars = FALSE, border= 'white',
          cex.axis = .6,
          ylab=c("Proportion of NAs", "Combinations"))
```

Based on the above: 

- `MFR` variable is missing about 8% of its values  
- `Filler.Speed` is missing about 2%  
- 28 other variables missing about 1% or less of their values  
- 3 variables appear to contain all their values (no NAs present) `Brand.Code`, `Pressure.Vacuum`, `Air.Pressurer`  


## Distributions

Next, the distributions of numerical response variable `PH`, the categorical predictor variable `Brand.Code` and remaining numerical predictor variables. 

```{r message=FALSE, warning=FALSE, include=FALSE}
print(dfSummary(train), file = '~/train_summary.html')
```

Additionally, using [`train` Summary Statistics](https://htmlpreview.github.io/?https://github.com/gabbypaola/DATA624/blob/main/Project%202/train_summary.html) the values, frequency of each values of the variables can be noted, as well as small visuals of thier distributions.

```{r message=FALSE, warning=FALSE}
summary(train)
```

Below is the distribution of the `Brand.Code` variable. The variable has 5 levels, one of which is empty and appears unlabeled. Of the codes, `Brand.Code` B has has the highest number of values, followed by D, C, A, and lastly the unlabeled `Brand.Code` observation.

```{r message=FALSE, warning=FALSE}
ggplot(train, aes(x=reorder(Brand.Code, Brand.Code, function(x)-length(x)))) +
geom_bar() +  labs(x='Brand.Code')+
labs(title= 'Brand.Code Distribution')+
   theme_minimal()
```

Next are the distributions for all the numeric variables.

```{r message=FALSE, warning=FALSE}
DataExplorer::plot_histogram(train, nrow = 3L, ncol = 4L)
```

```{r eval=FALSE, include=FALSE}
stats <- descr(train,
  headings = FALSE, #remove headings
  stats = "common",# most common descriptive statistics, default is all
  transpose = TRUE #allows for better display due to large amount of variables
  )
dfstats <- as.data.frame.matrix(stats)

dfstats$rightskew <- dfstats$Mean > dfstats$Median
dfstats$rightamount <- round(dfstats$Mean - dfstats$Median, 4)

dfstats$leftskew <- dfstats$Mean < dfstats$Median
dfstats$leftamount <- round(dfstats$Median- dfstats$Mean , 4)
```


From the above, variables in the training data set exhibit the follow skews in distribution:

- Relatively Normal Distributions:  `Carb.Pressure`, `Carb.Temp`, `Fill.Ounces`, `PC.Volume`, `PH` (response variable)

- Left-skew Distributions:  `Carb.Flow`, `Filler.Speed`, `Mnf.Flow`, `MFR`, `Bowl.Setpoint`, `Filler.Level`, `Hyd.Pressure2`, `Hyd.Pressure3`, `Usage.cont`, `Carb.Pressure1`, `Filler.Speed`

- Right-skew Distributions: `Pressure.Setpoint`, `Fill.Pressure`, `Hyd.Pressure1`, `Temperature`, `Carb.Volume`, `PSC`, `PSC.CO2`, `PSC.Fill`, `Balling`, `Density`, `Hyd.Pressure4`, `Air.Pressurer`, `Alch.Rel`, `Carb.Rel`, `Oxygen.Filler`, `Balling.Lvl`, `Pressure.Vacuum`


## Variable Correlations

The relationship between the variables are reviewed using a correlation plot in order to detect multicolliniarity within the training data set. Based on the below, it looks like multicolliniarity is an issue provided the correlations between a lot of the predictor variables.

```{r fig.height=10, fig.width=12, message=FALSE, warning=FALSE}
numeric_values <- train 

numeric_values<- numeric_values %>% 
  select_if(is.numeric) %>% 
  na.omit()

train_cor <- cor(numeric_values)

#train_cor_filtered <- cor(train_cor[,-findCorrelation(train_cor,cutoff = 0.9,exact = TRUE)])

corrplot(train_cor, tl.col = 'black', col=brewer.pal(n=10, name="RdYlBu"))
```

```{r eval=FALSE, include=FALSE}
#The features identified to have high pair-wise correlations are as follow:
#setdiff(colnames(train_cor), colnames(train_cor_filtered))
```


```{r message=FALSE, warning=FALSE}
ph_corr <- as.data.frame(cor(numeric_values[-1], numeric_values$PH))

ph_corr <- cbind("Predictor" = rownames(ph_corr), ph_corr)
rownames(ph_corr) <- 1:nrow(ph_corr) 

ph_corr <- ph_corr[-24,]

ggplot(ph_corr, aes(x=fct_reorder(factor(Predictor),V1), y = (V1))) +
  geom_col(position="dodge", fill="steelblue") +
  coord_flip()+
  labs(title="Correlations to pH",
    x="Predictors",
    y="Correlation Coefficient")+
    geom_text(aes(label = round(V1,2)), colour = "black", size = 3,
              position = position_stack(vjust = 0.6))+
   theme_minimal()
```

## Near-Zero Variance

Un-informative variables are detected using the `nearZeroVar` function. There is one variable, `Hyd.Pressure1` with near-zero variance which will be removed from in the pre-processing stage.

```{r message=FALSE, warning=FALSE}
nzv<- nearZeroVar(train, saveMetrics= TRUE)
filter(nzv, nzv=="TRUE")
```

## Update `Brand.Code`

Next, the empty value in the `Brand.Code` variable is updated with the string "Unknown". Additionally, the variable is updated to an unordered factor type variable. 

```{r message=FALSE, warning=FALSE}
#add name to empty string
train$Brand.Code[train$Brand.Code == ""] <- "Unknown"

#convert variable type to factor
train <- train %>% 
  dplyr::mutate(Brand.Code = factor(Brand.Code, 
                         levels = c('A','B','C','D', 'Unknown'), 
                         ordered = FALSE))
```


# Build Models

**Pre-Process Training Data for linear and non-linear models**

Pre-processing of the data is needed based on the distributions and missing values noted in the training data set. The training data for linear and non-linear needs to be normalized where as the data does not need normalization for tree-based models.

```{r message=FALSE, warning=FALSE}
set.seed(624)

#remove pH from the train data set in order to only transform the predictors
train_features <- train %>% 
  dplyr::select(-c(PH))

#remove nzv, correlated values, center and scale, apply BoxCox for normalization
preProc <- preProcess(train_features, method=c("knnImpute","nzv","corr",
                                               "center", "scale", "BoxCox"))

#get the transformed features
preProc_train <- predict(preProc, train_features)

#add the PH response variable to the preProcessed train features
preProc_train$PH <- train$PH 

#there are 4 NAs in the PH response variable, those need to be removed
preProc_train <- na.omit(preProc_train)

#partition data for evaluation
training_set <- createDataPartition(preProc_train$PH, p=0.8, list=FALSE)

train_data <- preProc_train[training_set,]
eval_data <- preProc_train[-training_set,]
```


## Linear Models

We consider these linear regression models: **multi-linear regression**, **partial least squares**, **AIC optimized** . We utilize the `train()` function for all three models, feeding the same datasets for X and Y, and specifying the proper model-building technique via the “method” variable.

Moreover, the `prediction()` function in combination with `postResample()`are used to generate summary statistics for how our model performed on the evaluation data:

### Multi-linear regression

First, a multi-linear regression is created to predict the pH response variable.  
  
Multiple linear regression is used to assess the relationship between two variables while taking into account the effect of other variables. By taking into account the effect of other variables, we cancel out the effect of these other variables in order to isolate and measure the relationship between the two variables of interest. This point is the main difference with simple linear regression.

```{r message=FALSE, warning=FALSE}
#Remove PH from sets to feed models
set.seed(222)
y_train <- subset(train_data, select = -c(PH))
y_test <- subset(eval_data, select = -c(PH))

#generate model
linear_model <- train(x= y_train, y= train_data$PH,
                      method='lm',
                      trControl=trainControl(method = "cv", number = 10))

#evaluate model
lmPred <- predict(linear_model, newdata = y_test)
lmResample <- postResample(pred=lmPred, obs = eval_data$PH)
```


### Partial Least Squares

Next PLS regression is performed on the data to predict PH. PLS was chosen given the multicolliniarity detected earlier in the exploratory data analysis phase.  
  
Partial least squares regression (PLS regression) is a statistical method that bears some relation to principal components regression; instead of finding hyperplanes of minimum variance between the response and independent variables, it finds a linear regression model by projecting the predicted variables and the observable variables to a new space.

```{r message=FALSE, warning=FALSE}
set.seed(222)
#generate model
pls_model <- train(y_train, train_data$PH,
                      method='pls',
                      metric='Rsquared',
                      tuneLength=10,
                      trControl=trainControl(method = "cv",  number = 10))
#evaluate model metrics
plsPred <-predict(pls_model, newdata=y_test)
plsReSample <- postResample(pred=plsPred, obs = eval_data$PH)
```


### Stepwise AIC optimized

Lastly, for our linear models, a stepwise AIC model is run using `stepAIC`. The stepwise regression is performed by specifying the direction parameter with "both."

Stepwise regression is a combination of both backward elimination and forward selection methods. Stepwise method is a modification of the forward selection approach and differs in that variables already in the model do not necessarily stay. As in forward selection, stepwise regression adds one variable to the model at a time. After a variable is added, however, stepwise regression checks all the variables already included again to see whether there is a need to delete any variable that does not provide an improvement to the model based on a certain criterion.
  
```{r message=FALSE, warning=FALSE}
set.seed(222)
#generate model
initial <- lm(PH ~ . , data = train_data)
AIC_model <- stepAIC(initial, direction = "both",
                     trace = 0)

#evaluate model metrics
AIC_Pred <-predict(AIC_model, newdata=y_test)
aicResample <- postResample(pred=AIC_Pred, obs=eval_data$PH)
```


### Linear Model Metrics

We need to verify model performance and identify the strongest performing model in our multi-linear regression subset. 

```{r message=FALSE, warning=FALSE}
display <- rbind("Linear Regression" = lmResample,
                 "Stepwise AIC" = aicResample,
                 "Partial Least Squares" = plsReSample)
display %>% kable() %>% kable_paper()
```


## Non-linear Models

Building non-linear models. We will try k-nearest neighbors (KNN), support vector machines (SVM), multivariate adaptive regression splines (MARS), and neural networks. These models are not based on simple linear combinations of the predictors.

### K-Nearest Neighbors
K-Nearest Neighbors simply predicts a new sample using the K-closest samples from the training set. The predicted response for the new sample is then the mean of the K neighbors’ responses. Predictors with the largest scales will contribute most to the distance between samples so centering and scaling the data during pre-processing is important.

```{r message=FALSE, warning=FALSE}
set.seed(624)
knnModel <- train(PH~., data = train_data, 
                  method = "knn",
                  preProc = c("center", "scale"), 
                  tuneLength = 10)

#knnModel

knnPred <- predict(knnModel, newdata = eval_data)
knn_metrics <- postResample(pred = knnPred, obs = eval_data$PH)
#knn_metrics
```


### Support Vector Machines
Support Vector Machines follow the framework of robust regression where we seek to minimize the effect of outliers on the regression equations. We find parameter estimates that minimize SSE by not squaring the residuals when they are very large. In addition samples that the model fits well have no effect on the regression equation. A threshold is set using resampling and a kernel function which specifies the relationship between predictors and outcome so that only poorly predicted points called support vectors are used to fit the line. The radial kernel we are using has an additional parameter which impacts the smoothness of the upper and lower boundary.

```{r message=FALSE, warning=FALSE}
set.seed(624)
tc <- trainControl(method = "cv",
                           number = 5,
                           classProbs = T)


svmModel <- train(PH~., data = train_data,
                    method = "svmRadial",
                    preProcess = c("BoxCox","center", "scale"),
                    trControl = tc,
                    tuneLength = 9)

#svmModel

svmPred <- predict(svmModel, newdata = eval_data)
svm_metrics <- postResample(pred = svmPred, obs = eval_data$PH)
#svm_metrics
```

### MARS
MARS uses surrogate features instead of the original predictors. However, whereas PLS and neural networks are based on linear combinations of the predictors, MARS creates two contrasted versions of a predictor to enter the model. MARS features breaks the predictor into two groups, a “hinge” function of the original based on a cut point that achieves the smallest error, and models linear relationships between the predictor and the outcome in each group. The new features are added to a basic linear regression model to estimate the slopes and intercepts.

```{r message=FALSE, warning=FALSE}
set.seed(624)
marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:38)

mars <- train(PH~., data = train_data,
                   method = "earth",
                   tuneGrid = marsGrid,
                   trControl = trainControl(method = "cv"))

#mars

marsPred <- predict(mars, newdata = eval_data)
mars_metrics <- postResample(pred = marsPred, obs = eval_data$PH)
#mars_metrics
```

Observed Vs. Predicted  - Non - Linear Models with Reduced Predictor Set.

```{r message=FALSE, warning=FALSE}
knnModel_pred <- knnModel %>% predict(eval_data)

# Model performance metrics
knn_Accuracy <- data.frame(
  Model = "k-Nearest Neighbors",
  RMSE = caret::RMSE(knnModel_pred,eval_data$PH),
  Rsquare = caret::R2(knnModel_pred,eval_data$PH))


pred_svm <- svmModel %>% predict(eval_data)
# Model SVM performance metrics
SMV_Acc <- data.frame(
  Model = "Support Vector Machine",
  RMSE = caret::RMSE(pred_svm, eval_data$PH),
  Rsquare = caret::R2(pred_svm, eval_data$PH)
)
#summary(marsTuned)
# Make MARS predictions
pred_mars <- mars %>% predict(eval_data)
# Model MARS performance metrics
MARS_Acc <- data.frame(
  Model = "MARS Tuned",
  RMSE = caret::RMSE(pred_mars, eval_data$PH),
  Rsquare = caret::R2(pred_mars, eval_data$PH)
)
names(MARS_Acc)[names(MARS_Acc) == 'y'] <- "Rsquare"
#rbind(knn_Accuracy,SMV_Acc,MARS_Acc)

### code for the plot
par(mar = c(4, 4, 4, 4))
par(mfrow=c(2,2))
plot(knnModel_pred, eval_data$PH, ylab="Observed", col = "red")
abline(0, 1, lwd=2)
plot(pred_svm, eval_data$PH, ylab="Observed", col = "dark green")
abline(0, 1, lwd=2)
plot(pred_mars, eval_data$PH, ylab="Observed", col = "blue")
abline(0, 1, lwd=2)
mtext("Observed Vs. Predicted  - Non - Linear Models with Reduced Predictor Set", side = 3, line = -2, outer = TRUE)
```

### Neural Networks
Neural networks, like partial least squares, the outcome is modeled by an intermediary set of unobserved variables. These hidden units are linear combinations of the original predictors, but, unlike PLS models, they are not estimated in a hierarchical fashion. There are no constraints that help define these linear combinations. Each unit must then be related to the outcome using another linear combination connecting the hidden units. Treating this model as a nonlinear regression model, the parameters are usually optimized using the back-propagation algorithm to minimize the sum of the squared residuals.

```{r message=FALSE, warning=FALSE}
set.seed(624)
NNModel <- avNNet(PH~., data = train_data,
                   size = 5, 
                   decay = 0.01,
                   linout = TRUE, 
                   trace = FALSE,
                   maxit = 500)

#NNModel

pred_NNModel <- NNModel %>% predict(eval_data)
nn_metrics <- postResample(pred = pred_NNModel, obs = eval_data$PH)
#nn_metrics
```

### Non-Linear Model Metrics
```{r message=FALSE, warning=FALSE}
rbind( "KNN" = knn_metrics,
       "SVM" = svm_metrics,
       "MARS" = mars_metrics,
       "Neural Network" = nn_metrics) %>% 
  kable() %>% kable_paper()
```
  
  
#### Select Neural Network as best nonlinear regression models
In the optimal nonlinear regression model was the Neural model with one of the lowest RMSE and higher R-squared. Below are the most important overall variables using the function `varImp`.


```{r message=FALSE, warning=FALSE}
varImp(NNModel) %>% head(10)
```


The top predictors in our best performing non-linear model with Neural Network.

Explore the relationships between the top predictors and the response for the predictors that are unique to the optimal nonlinear regression model. 

```{r message=FALSE, warning=FALSE}
ggplot(train_data, aes(Oxygen.Filler, PH)) +
  geom_point()
```


```{r message=FALSE, warning=FALSE}
ggplot(train_data, aes(Mnf.Flow	, PH)) +
  geom_point()
```
```{r message=FALSE, warning=FALSE}
ggplot(train_data, aes(Bowl.Setpoint	, PH)) +
  geom_point()
```

Checking the top nonlinear predictors overall:

- Oxygen.Filler 100 has positive correlation  
- Mnf.Flow 62 has negative correlation  
- Bowl.Setpoint 52 has positive correlation  


## Tree Based Models

**Pre-Process Training Data for Tree Based Models**

The training data is pre-processed differently for tree based models since they do not require the training data to be normalized.

```{r message=FALSE, warning=FALSE}
set.seed(624)

#remove pH from the train data set in order to only transform the predictors
train_features <- train %>% 
  dplyr::select(-c(PH))

#remove nzv, correlated values, impute NAs
preProc <- preProcess(train_features, method=c("knnImpute","nzv","corr"))

#get the transformed features
preProc_train <- predict(preProc, train_features)

#add the PH response variable to the preProcessed train features
preProc_train$PH <- train$PH 

#there are 4 NAs in the PH response variable, those need to be removed
preProc_train <- na.omit(preProc_train)

#partition data for evaluation
training_set <- createDataPartition(preProc_train$PH, p=0.8, list=FALSE)

train_data_rf <- preProc_train[training_set,]
eval_data_rf <- preProc_train[-training_set,]

train_rf_predictors <- train_data_rf[-c(26)]
eval_rf_predictors <- eval_data_rf[-c(26)]
```

### Random Forest

Below a random forest regression model is created to predict the desired response variable, PH.  
Random forest is a Supervised Machine Learning Algorithm that is used widely in Classification and Regression problems. It builds decision trees on different samples and takes their majority vote for classification and average in case of regression.

```{r message=FALSE, warning=FALSE}
set.seed(624)
#fit the model
rf_model <- randomForest(train_rf_predictors, train_data_rf$PH, 
                         importance = TRUE, ntrees = 500)
#metrics
rfPred <- predict(rf_model, newdata = eval_rf_predictors)
rf_metrics <- postResample(pred = rfPred, obs = eval_data_rf$PH)
#rf_metrics
```

### Boosted Trees

Next, boosted trees are also used to model the data using the `train` function and defining the `method` parameter with `gbm` to generate a prediction for the response variable PH.  

Boosted Trees are commonly used in regression. They are an ensemble method similar to bagging, however, instead of building multiple trees in parallel, they build tress sequentially. They use the previous tree to find errors and build a new tree by correcting the previous.

```{r message=FALSE, warning=FALSE}
set.seed(624)
#fit model
gbm_model <- train(train_rf_predictors, train_data_rf$PH,
    method = "gbm",
    verbose = FALSE)
#metrics
gbmPred <- predict(gbm_model, newdata = eval_rf_predictors)
gbm_metrics <- postResample(pred = gbmPred, obs = eval_data_rf$PH)
#gbm_metrics
```

### Cubist

Finally for the last Tree Based Model, a Cubist model is run also using the `train` function and defining the `method` parameter with `cubist`.  

In Cubist models, a tree is grown where the terminal leaves contain linear regression models. These models are based on the predictors used in previous splits. Also, there are intermediate linear models at each step of the tree. A prediction is made using the linear regression model at the terminal node of the tree, but is “smoothed” by taking into account the prediction from the linear model in the previous node of the tree (which also occurs recursively up the tree). The tree is reduced to a set of rules, which initially are paths from the top of the tree to the bottom. Rules are eliminated via pruning and/or combined for simplification.

```{r message=FALSE, warning=FALSE}
set.seed(6)
#fit model
cube_model <- train(train_rf_predictors, train_data_rf$PH,
    method = "cubist",
    verbose = FALSE)
#metrics
cubePred <- predict(cube_model, newdata =  eval_rf_predictors)
cube_metrics <- postResample(pred = cubePred, obs = eval_data_rf$PH)
#cube_metrics
```


### Tree Based Model Metrics 

Of the tree based models produced, the best performing model turned out to be the Cubist Model with an RMSE of 0.095, and an R-squared value of 0.685 indicating the percentage of the variance in the dependent variable, PH that the independent variables explain.

```{r message=FALSE, warning=FALSE}
rbind("Random Forest" = rf_metrics,
      "Boosted Trees" = gbm_metrics,
      "Cubist" = cube_metrics) %>% 
  kable() %>% kable_paper()
```


#  Evaluate & Select Models

Next, the metrics for each of the best performing Linear, Non-Linear, and Tree based models are complied to evaluate the performance of each and select the best model.

```{r message=FALSE, warning=FALSE}
all_metrics <- as.data.frame(rbind("Linear Regression" = lmResample,
      "Neural Network" = nn_metrics,
      "Cubist" = cube_metrics))

all_metrics[order(all_metrics$RMSE),] %>% 
  kable() %>% kable_paper()
```

Using the RMSE, we see the best performing model of those created is the Cubist model. Next we will proceed to evaluate the model and its important variables using `varImp`.

```{r message=FALSE, warning=FALSE}
plot(varImp(cube_model), top=10)
```


# Predict PH Values

Next, using the Cubist model which was identified as the best performing model above, the PH values are predicted with the provided `test` data set.

## Load and Review Test Data

First the data is loaded and reviewed. Using the `glimpse` function, we note the test data has a similar structure to that of the training data with `Brand.Code` appearing as `chr` type which will need to be checked for empty values and converted to unordered factor type. The PH response variable needs to be removed to run the model and will be subset from the features. 

```{r message=FALSE, warning=FALSE}
#load test data
test <- read.csv("TestData.csv")

#review 
glimpse(test)

#subset features from response
test_features <- test %>% 
  dplyr::select(-c(PH))
```

```{r message=FALSE, warning=FALSE, include=FALSE}
print(dfSummary(test), file = '~/test_summary.html')
```
Using the `summary` function in addition to the 
[`test` Summary Statistics](https://htmlpreview.github.io/?https://github.com/gabbypaola/DATA624/blob/main/Project%202/test_summary.html), the distributions and NA's are identified. 

```{r message=FALSE, warning=FALSE}
summary(test)
```

Next, for easy of visualization the `Brand.Code` is plotted to identify the spread of the factors and identify the empty values that will need to be updated. The distributions for the remaining features will not be added to the visualization as the distributions for tree based models do not require normalization in the pre-processing stage.

```{r message=FALSE, warning=FALSE}
ggplot(test, aes(x=reorder(Brand.Code, Brand.Code, function(x)-length(x)))) +
geom_bar() +  labs(x='Brand.Code')+
labs(title= 'Brand.Code Distribution')+
   theme_minimal()
```

Below the NAs in the test data set are visualized to easily see how many there are present. In the test data, about 10% of the missing values are within the `MFR` predictor variable. The NA values present in the test data will be imputed in the same fashion they were imputed in the training data by using the "knnImpute" `method` in the `preProcess` function of the caret library.

```{r message=FALSE, warning=FALSE, results=F}
#NAs in Test data set 
#NA counts by column
#sapply(test, function(x) sum(is.na(x)))

VIM::aggr(test_features, numbers=T, sortVars=T, bars = FALSE, border= 'white',
          cex.axis = .6,
          ylab=c("Proportion of NAs", "Combinations"))
```
  
In order to run our model using the Cubist model, the test data needs to be cleaned in a similar fashion to that of train data used to run the model. To do so, the `Brand.Code` predictor needs to converted to a factor and add an "Unknown" level for the empty values in the variable. 

```{r message=FALSE, warning=FALSE}
#prep data- change Brand.Code to factor
#add name to empty string
test_features$Brand.Code[test_features$Brand.Code == ""] <- "Unknown"

#convert variable type to factor
test_features <- test_features %>% 
  dplyr::mutate(Brand.Code = factor(Brand.Code, 
                         levels = c('A','B','C','D', 'Unknown'), 
                         ordered = FALSE))
```

The NAs identified in the predictors need to be imputed using the `preProcess` function. Additionally, near zero variance features and highly correlated variables were removed from the train data using the `preProcess` function. In order to ensure the same predictors in both the train and test data, the `preProcess` function will only be run with the `method` of "knnImpute," and variables eliminated from the train data will be identified and removed from the test data using `setdiff` and dplyr's `select` function, respectfully.

```{r message=FALSE, warning=FALSE}
set.seed(624)
#impute NAs
preProc_test <- preProcess(test_features, method=c("knnImpute"))

#get the transformed features
test_features <- predict(preProc_test, test_features)

#identify the variables that need to be removed 
setdiff(colnames(test_features),colnames(train_rf_predictors))

#remove the variables identified above
test_features <- test_features %>% 
  dplyr::select(-c("Hyd.Pressure1","Hyd.Pressure3","Filler.Level",
                   "Filler.Speed","Density","Balling","Balling.Lvl" ))
```


## Generate Predictions

Next, the predictions for PH are generated using the `predict` function along with the optimal model identified earlier as the Cubist model from the Tree Based models.

```{r message=FALSE, warning=FALSE}
predict <- round(predict(cube_model, newdata=test_features),2)

test_features$PH <- predict
```


## Generate Excel file

Finally, the predicted PH values along with the predictor variables are exported to a CSV file using `write_excel_csv`.

```{r message=FALSE, warning=FALSE}
#write_excel_csv(test_features, "Predictions.csv")
```


<!------- Below is for removing excessive space in Rmarkdown | HTML formatting -------->

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>
