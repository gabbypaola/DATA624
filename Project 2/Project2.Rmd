---
title: "DATA 624 | Predictive Analytics"
subtitle: "Project 2"
author: "Gehad Gad, Karim Hammoud, Gabriella Martinez"
date: "`r Sys.Date()`"
output:
      html_document:
        toc: yes
        toc_float: yes
        theme: yeti
        highlight: kate
        font-family: "Arial"
        code_folding: hide
---


# Instructions

This is role playing.  I am your new boss.  I am in charge of production at ABC Beverage and you are a team of data scientists reporting to me.  My leadership has told me that new regulations are requiring us to understand our manufacturing process, the **predictive factors and be able to report to them our predictive model of PH.**  

Please use the historical data set I am providing. Build and report the factors in BOTH a technical and non-technical report.  I like to use Word and Excel.  Please provide your non-technical report in a business friendly readable document and your predictions in an Excel readable format. The technical report should show clearly the models you tested and how you selected your final approach.  

Please submit both Rpubs links and .rmd files or other readable formats for technical and non-technical reports.  Also submit the excel file showing the prediction of your models for pH.
```{r message=FALSE, warning=FALSE}
library(ggplot2)
library(readr)
library(DataExplorer)
library(summarytools)
library(Amelia)
library(VIM)
library(dplyr)
library(forecast)
library(tidyr)
library(mice)
library(corrplot)
library(MASS)
library(earth)
library(RANN)
library(caret)
library(car)
library(forcats)
library(RColorBrewer)
library(randomForest)
library(gbm)
library(Cubist)
library(kableExtra)
library(e1071)
```

# Exploratory Data Analysis

## Load & Review Train Data
```{r message=FALSE, warning=FALSE}
#load data
train <- read.csv("TrainingData.csv")
#review
glimpse(train)
```

From the above it is noted that the `train` data set:

- 2571 observations with 33 columns (32 predictor variables)
- `Brand.Code` is character type and seems it is an unordered categorical variable which needs to be updated as such
- 4 predictor variables are integer type: `Hyd.Pressure4`, `Filler.Speed`, `Carb.Flow`, `Bowl.Setpoint`, remaining are float
- There are predictors with varying ranges for example: `Mnf. Flow` -100.20 to 229.40, `Carb.Flow` 26 to 5104, and `PSC` 0.002 to 0.270
- NAs detected in the first few observations

## Missing Values
```{r message=FALSE, warning=FALSE, results=F}
#NA counts by column
#sapply(train, function(x) sum(is.na(x)))
VIM::aggr(train, numbers=T, sortVars=T, bars = FALSE, border= 'white',
          cex.axis = .6,
          ylab=c("Proportion of NAs", "Combinations"))
```

Based on the above: 

- `MFR` variable is missing about 8% of its values  
- `Filler.Speed` is missing about 2%  
- 28 other variables missing about 1% or less of their values  
- 3 variables appear to contain all their values (no NAs present) `Brand.Code`, `Pressure.Vacuum`, `Air.Pressurer`  


## Distributions

Next, the distributions of numerical response variable `PH`, the categorical predictor variable `Brand.Code` and remaining numerical predictor variables. 

```{r message=FALSE, warning=FALSE, include=FALSE}
print(dfSummary(train), file = '~/train_summary.html')
```

Additionally, using [`train` Summary Statistics](https://htmlpreview.github.io/?https://github.com/gabbypaola/DATA624/blob/main/Project%202/train_summary.html) the values, frequency of each values of the variables can be noted, as well as small visuals of thier distributions.

```{r}
summary(train)
```

Below is the distribution of the `Brand.Code` variable. The variable has 5 levels, one of which is empty and appears unlabeled. Of the codes, `Brand.Code` B has has the highest number of values, followed by D, C, A, and lastly the unlabeled `Brand.Code` observation.

```{r}
ggplot(train, aes(x=reorder(Brand.Code, Brand.Code, function(x)-length(x)))) +
geom_bar() +  labs(x='Brand.Code')+
labs(title= 'Brand.Code Distribution')+
   theme_minimal()
```

Next are the distributions for all the numeric variables.
```{r message=FALSE, warning=FALSE}
DataExplorer::plot_histogram(train, nrow = 3L, ncol = 4L)
```

```{r eval=FALSE, include=FALSE}
stats <- descr(train,
  headings = FALSE, #remove headings
  stats = "common",# most common descriptive statistics, default is all
  transpose = TRUE #allows for better display due to large amount of variables
  )
dfstats <- as.data.frame.matrix(stats)

dfstats$rightskew <- dfstats$Mean > dfstats$Median
dfstats$rightamount <- round(dfstats$Mean - dfstats$Median, 4)

dfstats$leftskew <- dfstats$Mean < dfstats$Median
dfstats$leftamount <- round(dfstats$Median- dfstats$Mean , 4)
```


From the above, variables in the training data set exhibit the follow skews in distribution:

- normal:  `Carb.Pressure`, `Carb.Temp`, `Fill.Ounces`, `PC.Volume`, `PH` (response variable)

- left-skew:  `Carb.Flow`, `Filler.Speed`, `Mnf.Flow`, `MFR`, `Bowl.Setpoint`, `Filler.Level`, `Hyd.Pressure2`, `Hyd.Pressure3`, `Usage.cont`, `Carb.Pressure1`, `Filler.Speed`

- right-skew: `Pressure.Setpoint`, `Fill.Pressure`, `Hyd.Pressure1`, `Temperature`, `Carb.Volume`, `PSC`, `PSC.CO2`, `PSC.Fill`, `Balling`, `Density`, `Hyd.Pressure4`, `Air.Pressurer`, `Alch.Rel`, `Carb.Rel`, `Oxygen.Filler`, `Balling.Lvl`, `Pressure.Vacuum`


## Variable Correlations

The relationship between the variables are reviewed using a correlation plot in order to detect multicolliniarity within the training data set. Based on the below, it looks like multicolliniarity is an issue provided the correlations between a lot of the predictor variables.

```{r fig.height=10, fig.width=12, message=FALSE, warning=FALSE}
numeric_values <- train 

numeric_values<- numeric_values %>% 
  select_if(is.numeric) %>% 
  na.omit()

train_cor <- cor(numeric_values)

#train_cor_filtered <- cor(train_cor[,-findCorrelation(train_cor,cutoff = 0.9,exact = TRUE)])

corrplot(train_cor, tl.col = 'black', col=brewer.pal(n=10, name="RdYlBu"))
```

```{r eval=FALSE, include=FALSE}
#The features identified to have high pair-wise correlations are as follow:
#setdiff(colnames(train_cor), colnames(train_cor_filtered))
```


```{r}
ph_corr <- as.data.frame(cor(numeric_values[-1], numeric_values$PH))

ph_corr <- cbind("Predictor" = rownames(ph_corr), ph_corr)
rownames(ph_corr) <- 1:nrow(ph_corr) 

ph_corr <- ph_corr[-24,]

ggplot(ph_corr, aes(x=fct_reorder(factor(Predictor),V1), y = (V1))) +
  geom_col(position="dodge", fill="steelblue") +
  coord_flip()+
  labs(title="Correlations to pH",
    x="Predictors",
    y="Correlation Coefficient")+
    geom_text(aes(label = round(V1,2)), colour = "black", size = 3,
              position = position_stack(vjust = 0.6))+
   theme_minimal()
```

## Near-Zero Variance

Un-informative variables are detected using the `nearZeroVar` function. There is one variable, `Hyd.Pressure1` with near-zero variance which will be removed from in the pre-processing stage.

```{r}
nzv<- nearZeroVar(train, saveMetrics= TRUE)
filter(nzv, nzv=="TRUE")
```

## Update `Brand.Code`

Next, the empty value in the `Brand.Code` variable is updated with the string "Unknown". Additionally, the variable is updated to an unordered factor type variable. 

```{r}
#add name to empty string
train$Brand.Code[train$Brand.Code == ""] <- "Unknown"

#convert variable type to factor
train <- train %>% 
  dplyr::mutate(Brand.Code = factor(Brand.Code, 
                         levels = c('A','B','C','D', 'Unknown'), 
                         ordered = FALSE))
```


# Build Models

**Pre-Process Training Data for linear and non-linear models**

Pre-processing of the data is needed based on the distributions and missing values noted in the training data set. The training data for linear and non-linear needs to be normalized where as the data does not need normalization for random forest models.

```{r}
set.seed(624)

#remove pH from the train data set in order to only transform the predictors
train_features <- train %>% 
  dplyr::select(-c(PH))

#remove nzv, correlated values, center and scale, apply BoxCox for normalization
preProc <- preProcess(train_features, method=c("knnImpute","nzv","corr",
                                               "center", "scale", "BoxCox"))

#get the transformed features
preProc_train <- predict(preProc, train_features)

#add the PH response variable to the preProcessed train features
preProc_train$PH <- train$PH 

#there are 4 NAs in the PH response variable, those need to be removed
preProc_train <- na.omit(preProc_train)

#partition data for evaluation
training_set <- createDataPartition(preProc_train$PH, p=0.8, list=FALSE)

train_data <- preProc_train[training_set,]
eval_data <- preProc_train[-training_set,]
```


## Linear Models

We consider these linear regression models: **multi-linear regression**, **partial least squares**, **AIC optimized** . We utilize train() function for all three models, feeding the same datasets for X and Y, and specifying the proper model-building technique via the “method” variable.

### Multi-linear regression

```{r}
#Remove PH from sets to feed models
set.seed(222)
y_train <- subset(train_data, select = -c(PH))
y_test <- subset(eval_data, select = -c(PH))

linear_model <- train(x= y_train, y= train_data$PH,
                      method='lm',
                      trControl=trainControl(method = "cv", number = 10))
```

**Prediction**

We use the prediction() function in combination with postResample() to generate summary statistics for how our model performed on unseen, test data:

```{r}
lmPred <- predict(linear_model, newdata = y_test)
lmResample <- postResample(pred=lmPred, obs = eval_data$PH)
```

### Partial Least Squares

```{r}
pls_model <- train(y_train, train_data$PH,
                      method='pls',
                      metric='Rsquared',
                      tuneLength=10,
                      trControl=trainControl(method = "cv",  number = 10))
```

**Prediction**

```{r}
set.seed(222)
plsPred <-predict(pls_model, newdata=y_test)
plsReSample <- postResample(pred=plsPred, obs = eval_data$PH)
```

### AIC optimized

```{r}
initial <- lm(PH ~ . , data = train_data)

AIC_model <- stepAIC(initial, direction = "both")
```

**Prediction**

```{r}
AIC_Pred <-predict(AIC_model, newdata=y_test)
aicResample <- postResample(pred=AIC_Pred, obs=eval_data$PH)
```

We need to verify model performance and identify the strongest performing model in our multi-linear regression subset. 

```{r}
display <- rbind(
"Linear Regression" = lmResample,
"Stepwise AIC" = aicResample,
"Partial Least Squares" = plsReSample
)
display %>% kable() %>% kable_paper()
```


## Non-linear Models - Karim

Building non-linear models. We will try k-nearest neighbors (KNN), support vector machines (SVM), multivariate adaptive regression splines (MARS), and neural networks. These models are not based on simple linear combinations of the predictors.

K-Nearest Neighbors simply predicts a new sample using the K-closest samples from the training set. The predicted response for the new sample is then the mean of the K neighbors’ responses. Predictors with the largest scales will contribute most to the distance between samples so centering and scaling the data during pre-processing is important.

```{r}
knnModel <- train(PH~., data = train_data,
method = "knn",
preProc = c("center", "scale"), 
tuneLength = 10)


```

Support Vector Machines follow the framework of robust regression where we seek to minimize the effect of outliers on the regression equations. We find parameter estimates that minimize SSE by not squaring the residuals when they are very large. In addition samples that the model fits well have no effect on the regression equation. A threshold is set using resampling and a kernel function which specifies the relationship between predictors and outcome so that only poorly predicted points called support vectors are used to fit the line. The radial kernel we are using has an additional parameter which impacts the smoothness of the upper and lower boundary.

```{r}
tc <- trainControl(method = "cv",
                           number = 5,
                           classProbs = T)


svmModel <- train(PH~., data = train_data,
                    method = "svmRadial",
                    preProcess = c("BoxCox","center", "scale"),
                    trControl = tc,
                    tuneLength = 9)
```

MARS uses surrogate features instead of the original predictors. However, whereas PLS and neural networks are based on linear combinations of the predictors, MARS creates two contrasted versions of a predictor to enter the model. MARS features breaks the predictor into two groups, a “hinge” function of the original based on a cut point that achieves the smallest error, and models linear relationships between the predictor and the outcome in each group. The new features are added to a basic linear regression model to estimate the slopes and intercepts.

```{r}

marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:38)

mars <- train(PH~., data = train_data,
                   method = "earth",
                   tuneGrid = marsGrid,
                   trControl = trainControl(method = "cv"))
```


Now lets build a matrix for the three different  non linear models 

```{r}
knnModel_pred <- knnModel %>% predict(eval_data)
# Model performance metrics
knn_Accuracy <- data.frame(
  Model = "k-Nearest Neighbors",
  RMSE = caret::RMSE(knnModel_pred,eval_data$PH),
  Rsquare = caret::R2(knnModel_pred,eval_data$PH))


pred_svm <- svmModel %>% predict(eval_data)
# Model SVM performance metrics
SMV_Acc <- data.frame(
  Model = "Support Vector Machine",
  RMSE = caret::RMSE(pred_svm, eval_data$PH),
  Rsquare = caret::R2(pred_svm, eval_data$PH)
)
#summary(marsTuned)
# Make MARS predictions
pred_mars <- mars %>% predict(eval_data)
# Model MARS performance metrics
MARS_Acc <- data.frame(
  Model = "MARS Tuned",
  RMSE = caret::RMSE(pred_mars, eval_data$PH),
  Rsquare = caret::R2(pred_mars, eval_data$PH)
)
names(MARS_Acc)[names(MARS_Acc) == 'y'] <- "Rsquare"
rbind(knn_Accuracy,SMV_Acc,MARS_Acc)
```

Now I will plot the Observed Vs. Predicted  - Non - Linar Models with Reduced Predictor Set.

```{r}
par(mar = c(4, 4, 4, 4))
par(mfrow=c(2,2))
plot(knnModel_pred, eval_data$PH, ylab="Observed", col = "red")
abline(0, 1, lwd=2)
plot(pred_svm, eval_data$PH, ylab="Observed", col = "dark green")
abline(0, 1, lwd=2)
plot(pred_mars, eval_data$PH, ylab="Observed", col = "blue")
abline(0, 1, lwd=2)
mtext("Observed Vs. Predicted  - Non - Linar Models with Reduced Predictor Set", side = 3, line = -2, outer = TRUE)
```
Neural networks, like partial least squares, the outcome is modeled by an intermediary set of unobserved variables. These hidden units are linear combinations of the original predictors, but, unlike PLS models, they are not estimated in a hierarchical fashion. There are no constraints that help define these linear combinations. Each unit must then be related to the outcome using another linear combination connecting the hidden units. Treating this model as a nonlinear regression model, the parameters are usually optimized using the back-propagation algorithm to minimize the sum of the squared residuals.

```{r}
NNModel <- avNNet(PH~., data = train_data,
                   size = 5, 
                   decay = 0.01,
                   linout = TRUE, 
                   trace = FALSE,
                   maxit = 500)
pred_NNModel <- NNModel %>% predict(eval_data)

# Model MARS performance metrics

predictions_NNModel_1_Acc <- data.frame(
  Model = "Neural Network",
  RMSE = caret::RMSE(pred_NNModel, eval_data$PH),
  Rsquare = caret::R2(pred_NNModel, eval_data$PH)
)

```

In the optimal nonlinear regression models I can see SVM has the best predictors with the best R squared score, now I will check the most important variabels in SVB by checking the overall imprtant using the function varImp


```{r}
varImp(svmModel, 10)
```


The top predictors in our best performing non-linear model with (Support Vector Machine (SVM))

Explore the relationships between the top predictors and the response for the predictors that are unique to the optimal nonlinear regression model. 


```{r}
ggplot(train_data, aes(Oxygen.Filler, PH)) +
  geom_point()
```


```{r}
ggplot(train_data, aes(Mnf.Flow	, PH)) +
  geom_point()
```
```{r}
ggplot(train_data, aes(Bowl.Setpoint	, PH)) +
  geom_point()
```

Checking the top predictors in SVB as the below overall:

Oxygen.Filler 100 has positive correlation
Mnf.Flow 62 has negative correlation
Bowl.Setpoint 52 has positive correlation


## Tree Based Models

**Pre-Process Training Data for Tree Based Models**

The training data is pre-processed differently for tree based models since they do not require the training data to be normalized.

```{r}
set.seed(624)

#remove pH from the train data set in order to only transform the predictors
train_features <- train %>% 
  dplyr::select(-c(PH))

#remove nzv, correlated values
preProc <- preProcess(train_features, method=c("knnImpute","nzv","corr"))

#get the transformed features
preProc_train <- predict(preProc, train_features)

#add the PH response variable to the preProcessed train features
preProc_train$PH <- train$PH 

#there are 4 NAs in the PH response variable, those need to be removed
preProc_train <- na.omit(preProc_train)

#partition data for evaluation
training_set <- createDataPartition(preProc_train$PH, p=0.8, list=FALSE)

train_data_rf <- preProc_train[training_set,]
eval_data_rf <- preProc_train[-training_set,]

train_rf_predictors <- train_data_rf[-c(26)]
eval_rf_predictors <- eval_data_rf[-c(26)]
```

### Random Forest
```{r}
set.seed(624)
#fit the model
rf_model <- randomForest(train_rf_predictors, train_data_rf$PH, importance = TRUE, ntrees = 1000)
rf_model

rfPred <- predict(rf_model, newdata = eval_rf_predictors)
rf_metrics <- postResample(pred = rfPred, obs = eval_data_rf$PH)
rf_metrics
```
### Tuned Random Forest
```{r}
set.seed(624)

trControl <- trainControl(method = "cv",
    search = "grid")

rf_default <- train(PH~.,
    data = train_data_rf,
    method = "rf",
    trControl = trControl)
# Print the results
print(rf_default)
```

```{r}
prediction_default <-predict(rf_default, eval_data_rf)
postResample(pred = prediction_default, obs = eval_data_rf$PH)
```


```{r}
#search best mtry
set.seed(1234)
tuneGrid <- expand.grid(.mtry = c(25:35))
rf_mtry <- train(PH~.,
    data = train_data_rf,
    method = "rf",
    tuneGrid = tuneGrid,
    trControl = trControl,
    importance = TRUE,
    ntree = 300)
print(rf_mtry)
```


```{r}
best_mtry <- rf_mtry$bestTune$mtry 
best_mtry
```


```{r}
#best maxnodes
# 5:30: lowest mean RMSE=30; 25:45 lowest mean RMSE = 45
store_maxnode <- list()
tuneGrid <- expand.grid(.mtry = best_mtry)
for (maxnodes in c(25:45)) {
    set.seed(1234)
    rf_maxnode <- train(PH~.,
    data = train_data_rf,
        method = "rf",
        tuneGrid = tuneGrid,
        trControl = trControl,
        importance = TRUE,
        maxnodes = maxnodes,
        ntree = 300)
    key <- toString(maxnodes)
    store_maxnode[[key]] <- rf_maxnode
}
results_node <- resamples(store_maxnode)
summary(results_node)

```



```{r}
#best ntrees
store_maxtrees <- list()
for (ntree in c(250, 300, 350, 400, 450, 500, 550, 600, 800, 1000, 2000)) {
    set.seed(5678)
    rf_maxtrees <- train(PH~.,
    data = train_data_rf,
        method = "rf",
        tuneGrid = tuneGrid,
        trControl = trControl,
        importance = TRUE,
        maxnodes = 50,
        ntree = ntree)
    key <- toString(ntree)
    store_maxtrees[[key]] <- rf_maxtrees
}
results_tree <- resamples(store_maxtrees)
summary(results_tree)

#2000 trees
```

```{r}
#train rf model with the tuning parameters
fit_rf <- train(PH~.,
    data = train_data_rf,
    method = "rf",
    tuneGrid = tuneGrid,
    trControl = trControl,
    importance = TRUE,
    ntree = 2000,
    maxnodes = 45)
```

```{r}
prediction <-predict(fit_rf, eval_data_rf)
postResample(pred = prediction, obs = eval_data_rf$PH)
```


### Boosted Trees
```{r}
set.seed(624)
gbm_model <- gbm.fit(train_rf_predictors, train_data_rf$PH, distribution = "gaussian")
gbm_model
gbmPred <- predict(gbm_model, newdata = eval_rf_predictors)
gbm_metrics <- postResample(pred = gbmPred, obs = eval_data_rf$PH)
gbm_metrics
```

### Cubist
```{r}
set.seed(624)
cube_model <- cubist(train_rf_predictors, train_data_rf$PH)
cube_model
cubePred <- predict(cube_model, newdata =  eval_rf_predictors)
cube_metrics <- postResample(pred = cubePred, obs = eval_data_rf$PH)
cube_metrics
```

```{r}
display <- rbind(
"Random Forest" = rf_metrics,
"Boosted Trees" = gbm_metrics,
"Cubist" = cube_metrics
)
display %>% kable() %>% kable_paper()
```

#  Evaluate & Select Models

# Predict PH Values

## Load and Review Test Data
```{r message=FALSE, warning=FALSE}
#load test data
test <- read.csv("TestData.csv")
#review 
glimpse(test)
```

```{r message=FALSE, warning=FALSE, results=F}
#NAs in Test data set 
#NA counts by column
#sapply(test, function(x) sum(is.na(x)))
VIM::aggr(test, numbers=T, sortVars=T, bars = FALSE, border= 'white',
          cex.axis = .6,
          ylab=c("Proportion of NAs", "Combinations"))
```

```{r message=FALSE, warning=FALSE, include=FALSE}
print(dfSummary(test), file = '~/test_summary.html')
```
[`test` Summary Statistics](https://htmlpreview.github.io/?https://github.com/gabbypaola/DATA624/blob/main/Project%202/test_summary.html)

## Generate Predictions

## Generate Excel file

```{r}
#write_excel_csv(name_of_datafram, "name_of_csv_file.csv")
```


<!------- Below is for removing excessive space in Rmarkdown | HTML formatting -------->

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>
