---
title: "DATA 624 | Predictive Analytics"
subtitle: "Project 2"
author: "Gehad Gad, Karim Hammoud, Gabriella Martinez"
date: "`r Sys.Date()`"
output:
      html_document:
        toc: yes
        toc_float: yes
        theme: yeti
        highlight: kate
        font-family: "Arial"
        code_folding: hide
---


# Instructions

This is role playing.  I am your new boss.  I am in charge of production at ABC Beverage and you are a team of data scientists reporting to me.  My leadership has told me that new regulations are requiring us to understand our manufacturing process, the **predictive factors and be able to report to them our predictive model of PH.**  

Please use the historical data set I am providing. Build and report the factors in BOTH a technical and non-technical report.  I like to use Word and Excel.  Please provide your non-technical report in a business friendly readable document and your predictions in an Excel readable format. The technical report should show clearly the models you tested and how you selected your final approach.  

Please submit both Rpubs links and .rmd files or other readable formats for technical and non-technical reports.  Also submit the excel file showing the prediction of your models for pH.
```{r message=FALSE, warning=FALSE}
library(ggplot2)
library(readr)
library(DataExplorer)
library(summarytools)
library(Amelia)
library(VIM)
library(dplyr)
library(forecast)
library(tidyr)
library(mice)
library(corrplot)
library(MASS)
library(earth)
library(RANN)
library(caret)
library(car)
library(forcats)
library(RColorBrewer)
library(randomForest)
library(gbm)
library(Cubist)
library(kableExtra)
```

# Exploratory Data Analysis

## Load & Review Train Data
```{r message=FALSE, warning=FALSE}
#load data
train <- read.csv("TrainingData.csv")
#review
glimpse(train)
```

From the above it is noted that the `train` data set:

- 2571 observations with 33 columns (32 predictor variables)
- `Brand.Code` is character type and seems it is an unordered categorical variable which needs to be updated as such
- 4 predictor variables are integer type: `Hyd.Pressure4`, `Filler.Speed`, `Carb.Flow`, `Bowl.Setpoint`, remaining are float
- There are predictors with varying ranges for example: `Mnf. Flow` -100.20 to 229.40, `Carb.Flow` 26 to 5104, and `PSC` 0.002 to 0.270
- NAs detected in the first few observations

## Missing Values
```{r message=FALSE, warning=FALSE, results=F}
#NA counts by column
#sapply(train, function(x) sum(is.na(x)))
VIM::aggr(train, numbers=T, sortVars=T, bars = FALSE, border= 'white',
          cex.axis = .6,
          ylab=c("Proportion of NAs", "Combinations"))
```

Based on the above: 

- `MFR` variable is missing about 8% of its values  
- `Filler.Speed` is missing about 2%  
- 28 other variables missing about 1% or less of their values  
- 3 variables appear to contain all their values (no NAs present) `Brand.Code`, `Pressure.Vacuum`, `Air.Pressurer`  


## Distributions

Next, the distributions of numerical response variable `PH`, the categorical predictor variable `Brand.Code` and remaining numerical predictor variables. 

```{r message=FALSE, warning=FALSE, include=FALSE}
print(dfSummary(train), file = '~/train_summary.html')
```

Additionally, using [`train` Summary Statistics](https://htmlpreview.github.io/?https://github.com/gabbypaola/DATA624/blob/main/Project%202/train_summary.html) the values, frequency of each values of the variables can be noted, as well as small visuals of thier distributions.

```{r}
summary(train)
```

Below is the distribution of the `Brand.Code` variable. The variable has 5 levels, one of which is empty and appears unlabeled. Of the codes, `Brand.Code` B has has the highest number of values, followed by D, C, A, and lastly the unlabeled `Brand.Code` observation.

```{r}
ggplot(train, aes(x=reorder(Brand.Code, Brand.Code, function(x)-length(x)))) +
geom_bar() +  labs(x='Brand.Code')+
labs(title= 'Brand.Code Distribution')+
   theme_minimal()
```

Next are the distributions for all the numeric variables.
```{r message=FALSE, warning=FALSE}
DataExplorer::plot_histogram(train, nrow = 3L, ncol = 4L)
```

```{r eval=FALSE, include=FALSE}
stats <- descr(train,
  headings = FALSE, #remove headings
  stats = "common",# most common descriptive statistics, default is all
  transpose = TRUE #allows for better display due to large amount of variables
  )
dfstats <- as.data.frame.matrix(stats)

dfstats$rightskew <- dfstats$Mean > dfstats$Median
dfstats$rightamount <- round(dfstats$Mean - dfstats$Median, 4)

dfstats$leftskew <- dfstats$Mean < dfstats$Median
dfstats$leftamount <- round(dfstats$Median- dfstats$Mean , 4)
```


From the above, variables in the training data set exhibit the follow skews in distribution:

- normal:  `Carb.Pressure`, `Carb.Temp`, `Fill.Ounces`, `PC.Volume`, `PH` (response variable)

- left-skew:  `Carb.Flow`, `Filler.Speed`, `Mnf.Flow`, `MFR`, `Bowl.Setpoint`, `Filler.Level`, `Hyd.Pressure2`, `Hyd.Pressure3`, `Usage.cont`, `Carb.Pressure1`, `Filler.Speed`

- right-skew: `Pressure.Setpoint`, `Fill.Pressure`, `Hyd.Pressure1`, `Temperature`, `Carb.Volume`, `PSC`, `PSC.CO2`, `PSC.Fill`, `Balling`, `Density`, `Hyd.Pressure4`, `Air.Pressurer`, `Alch.Rel`, `Carb.Rel`, `Oxygen.Filler`, `Balling.Lvl`, `Pressure.Vacuum`


## Variable Correlations

The relationship between the variables are reviewed using a correlation plot in order to detect multicolliniarity within the training data set. Based on the below, it looks like multicolliniarity is an issue provided the correlations between a lot of the predictor variables.

```{r fig.height=10, fig.width=12, message=FALSE, warning=FALSE}
numeric_values <- train 

numeric_values<- numeric_values %>% 
  select_if(is.numeric) %>% 
  na.omit()

train_cor <- cor(numeric_values)

#train_cor_filtered <- cor(train_cor[,-findCorrelation(train_cor,cutoff = 0.9,exact = TRUE)])

corrplot(train_cor, tl.col = 'black', col=brewer.pal(n=10, name="RdYlBu"))
```

```{r eval=FALSE, include=FALSE}
#The features identified to have high pair-wise correlations are as follow:
#setdiff(colnames(train_cor), colnames(train_cor_filtered))
```


```{r}
ph_corr <- as.data.frame(cor(numeric_values[-1], numeric_values$PH))

ph_corr <- cbind("Predictor" = rownames(ph_corr), ph_corr)
rownames(ph_corr) <- 1:nrow(ph_corr) 

ph_corr <- ph_corr[-24,]

ggplot(ph_corr, aes(x=fct_reorder(factor(Predictor),V1), y = (V1))) +
  geom_col(position="dodge", fill="steelblue") +
  coord_flip()+
  labs(title="Correlations to pH",
    x="Predictors",
    y="Correlation Coefficient")+
    geom_text(aes(label = round(V1,2)), colour = "black", size = 3,
              position = position_stack(vjust = 0.6))+
   theme_minimal()
```

## Near-Zero Variance

Un-informative variables are detected using the `nearZeroVar` function. There is one variable, `Hyd.Pressure1` with near-zero variance which will be removed from in the pre-processing stage.

```{r}
nzv<- nearZeroVar(train, saveMetrics= TRUE)
filter(nzv, nzv=="TRUE")
```

## Update `Brand.Code`

Next, the empty value in the `Brand.Code` variable is updated with the string "Unknown". Additionally, the variable is updated to an unordered factor type variable. 

```{r}
#add name to empty string
train$Brand.Code[train$Brand.Code == ""] <- "Unknown"

#convert variable type to factor
train <- train %>% 
  dplyr::mutate(Brand.Code = factor(Brand.Code, 
                         levels = c('A','B','C','D', 'Unknown'), 
                         ordered = FALSE))
```


# Build Models

**Pre-Process Training Data**

Pre-processing of the data is needed based on the distributions and missing values noted in the training data set. The training data for linear and non-linear needs to be normalized where as the data does not need normalization for random forest models.

```{r}
set.seed(624)

#remove pH from the train data set in order to only transform the predictors
train_features <- train %>% 
  dplyr::select(-c(PH))

#remove nzv, correlated values, center and scale, apply BoxCox for normalization
preProc <- preProcess(train_features, method=c("knnImpute","nzv","corr",
                                               "center", "scale", "BoxCox"))

#get the transformed features
preProc_train <- predict(preProc, train_features)

#add the PH response variable to the preProcessed train features
preProc_train$PH <- train$PH 

#there are 4 NAs in the PH response variable, those need to be removed
preProc_train <- na.omit(preProc_train)

#partition data for evaluation
training_set <- createDataPartition(preProc_train$PH, p=0.8, list=FALSE)

train_data <- preProc_train[training_set,]
eval_data <- preProc_train[-training_set,]
```


## Linear Models

We consider these linear regression models: **multi-linear regression**, **partial least squares**, **AIC optimized** . We utilize train() function for all three models, feeding the same datasets for X and Y, and specifying the proper model-building technique via the “method” variable.

### Multi-linear regression

```{r}
#Remove PH from sets to feed models
set.seed(222)
y_train <- subset(train_data, select = -c(PH))
y_test <- subset(eval_data, select = -c(PH))

linear_model <- train(x= y_train, y= train_data$PH,
                      method='lm',
                      trControl=trainControl(method = "cv", number = 10))
```

**Prediction**

We use the prediction() function in combination with postResample() to generate summary statistics for how our model performed on unseen, test data:

```{r}
lmPred <- predict(linear_model, newdata = y_test)
lmResample <- postResample(pred=lmPred, obs = eval_data$PH)
```

### Partial Least Squares

```{r}
pls_model <- train(y_train, train_data$PH,
                      method='pls',
                      metric='Rsquared',
                      tuneLength=10,
                      trControl=trainControl(method = "cv",  number = 10))
```

**Prediction**

```{r}
set.seed(222)
plsPred <-predict(pls_model, newdata=y_test)
plsReSample <- postResample(pred=plsPred, obs = eval_data$PH)
```

### AIC optimized

```{r}
initial <- lm(PH ~ . , data = train_data)

AIC_model <- stepAIC(initial, direction = "both")
```

**Prediction**

```{r}
AIC_Pred <-predict(AIC_model, newdata=y_test)
aicResample <- postResample(pred=AIC_Pred, obs=eval_data$PH)
```

We need to verify model performance and identify the strongest performing model in our multi-linear regression subset. 

```{r}
display <- rbind(
"Linear Regression" = lmResample,
"Stepwise AIC" = aicResample,
"Partial Least Squares" = plsReSample
)
display %>% kable() %>% kable_paper()
```


## Non-linear Models

## Tree Based Models

**Pre-Process Training Data for Tree Based Models**

The training data is pre-processed differently for tree based models since they do not require the training data to be normalized.

```{r}
set.seed(624)

#remove pH from the train data set in order to only transform the predictors
train_features <- train %>% 
  dplyr::select(-c(PH))

#remove nzv, correlated values
preProc <- preProcess(train_features, method=c("knnImpute","nzv","corr"))

#get the transformed features
preProc_train <- predict(preProc, train_features)

#add the PH response variable to the preProcessed train features
preProc_train$PH <- train$PH 

#there are 4 NAs in the PH response variable, those need to be removed
preProc_train <- na.omit(preProc_train)

#partition data for evaluation
training_set <- createDataPartition(preProc_train$PH, p=0.8, list=FALSE)

train_data_rf <- preProc_train[training_set,]
eval_data_rf <- preProc_train[-training_set,]

train_rf_predictors <- train_data_rf[-c(26)]
eval_rf_predictors <- eval_data_rf[-c(26)]
```

### Random Forest
```{r}
set.seed(624)
#fit the model
rf_model <- randomForest(train_rf_predictors, train_data_rf$PH, importance = TRUE, ntrees = 1000)
rf_model

rfPred <- predict(rf_model, newdata = eval_rf_predictors)
rf_metrics <- postResample(pred = rfPred, obs = eval_data_rf$PH)
rf_metrics
```

### Boosted Trees
```{r}
set.seed(624)
gbm_model <- gbm.fit(train_rf_predictors, train_data_rf$PH, distribution = "gaussian")
gbm_model
gbmPred <- predict(gbm_model, newdata = eval_rf_predictors)
gbm_metrics <- postResample(pred = gbmPred, obs = eval_data_rf$PH)
gbm_metrics
```

### Cubist
```{r}
set.seed(624)
cube_model <- cubist(train_rf_predictors, train_data_rf$PH)
cube_model
cubePred <- predict(cube_model, newdata =  eval_rf_predictors)
cube_metrics <- postResample(pred = cubePred, obs = eval_data_rf$PH)
cube_metrics
```

```{r}
display <- rbind(
"Random Forest" = rf_metrics,
"Boosted Trees" = gbm_metrics,
"Cubist" = cube_metrics
)
display %>% kable() %>% kable_paper()
```

#  Evaluate & Select Models

# Predict PH Values

## Load and Review Test Data
```{r message=FALSE, warning=FALSE}
#load test data
test <- read.csv("TestData.csv")
#review 
glimpse(test)
```

```{r message=FALSE, warning=FALSE, results=F}
#NAs in Test data set 
#NA counts by column
#sapply(test, function(x) sum(is.na(x)))
VIM::aggr(test, numbers=T, sortVars=T, bars = FALSE, border= 'white',
          cex.axis = .6,
          ylab=c("Proportion of NAs", "Combinations"))
```

```{r message=FALSE, warning=FALSE, include=FALSE}
print(dfSummary(test), file = '~/test_summary.html')
```
[`test` Summary Statistics](https://htmlpreview.github.io/?https://github.com/gabbypaola/DATA624/blob/main/Project%202/test_summary.html)

## Generate Predictions

## Generate Excel file

```{r}
#write_excel_csv(name_of_datafram, "name_of_csv_file.csv")
```


<!------- Below is for removing excessive space in Rmarkdown | HTML formatting -------->

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>
